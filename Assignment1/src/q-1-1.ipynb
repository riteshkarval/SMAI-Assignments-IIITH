{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train decision tree only on categorical data. Report precision,recall, f1 score and accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Necessary imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split \n",
    "dataset = pd.read_csv(\"../input_data/train.csv\") \n",
    "dataset = dataset[['promotion_last_5years', 'Work_accident', 'sales', 'salary','left']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to calculate entropy\n",
    "Parameter is column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(column):\n",
    "    elements,counts = np.unique(column,return_counts = True)\n",
    "    entropy = 0\n",
    "    for i in range(len(elements)):\n",
    "        entropy += -(counts[i]/np.sum(counts))*(np.log2(counts[i]/np.sum(counts)))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to calculate information gain from the column with repect to the target column.\n",
    "Parameters are dataset, column name, column name of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data,split,target=\"left\"):\n",
    "    total_entropy = entropy(data[target])\n",
    "    vals,counts= np.unique(data[split],return_counts=True)\n",
    "    Weighted_Entropy = 0\n",
    "    for i in range(len(vals)):\n",
    "        weight = counts[i]/np.sum(counts)\n",
    "        ent = entropy(data.where(data[split]==vals[i]).dropna()[target])\n",
    "        Weighted_Entropy += weight*ent\n",
    "    InfoGain = total_entropy - Weighted_Entropy\n",
    "    return InfoGain"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to divide dataset into trian, validate and test sets.\n",
    "Size of train dataset is 60% of the data, rest 40% has been divided equally into validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(dataset):\n",
    "    size = len(dataset)\n",
    "    tsize = int(size*0.6)\n",
    "    vsize = int(size*0.8)\n",
    "    training_data = dataset.iloc[:tsize].reset_index(drop=True)\n",
    "    validation_data = dataset.iloc[tsize:vsize].reset_index(drop=True)\n",
    "    testing_data = dataset.iloc[vsize:].reset_index(drop=True)\n",
    "    return training_data,validation_data,testing_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to create the tree, it uses the ID3(https://en.wikipedia.org/wiki/ID3_algorithm) decision tree training algorithm.\n",
    "Prameters to the function are data(subdata if not root), data, column names in dataset, \n",
    "target label, parent of curent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createtree(subdata,data,attributes,label=\"left\",parent = None):\n",
    "    if len(np.unique(subdata[label])) <= 1:\n",
    "        return np.unique(subdata[label])[0]\n",
    "    elif len(subdata)==0:\n",
    "        return np.unique(data[label])[np.argmax(np.unique(data[label],return_counts=True)[1])]  \n",
    "    elif len(attributes) ==0:\n",
    "        return parent\n",
    "    else:\n",
    "        parent = np.unique(subdata[label])[np.argmax(np.unique(subdata[label],return_counts=True)[1])]\n",
    "        item = [InfoGain(subdata,attribute,label) for attribute in attributes] \n",
    "        selected_attribute_index = np.argmax(item)\n",
    "        selected_attribute = attributes[selected_attribute_index]\n",
    "        tree = {selected_attribute:{}}\n",
    "        attributes = [i for i in attributes if i != selected_attribute]\n",
    "        for value in np.unique(subdata[selected_attribute]):\n",
    "            value = value\n",
    "            sub_data = subdata.where(subdata[selected_attribute] == value).dropna()\n",
    "            subtree = createtree(sub_data,dataset,attributes,label,parent)\n",
    "            tree[selected_attribute][value] = subtree\n",
    "        return(tree)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to predict the outcome of sample row. \n",
    "parameters are sample(row of attributes), tree(build from create tree function)\n",
    "Query should be in a dictionary format ex: {'promotion_last_5years': 0,'Work_accident': 0,'sales': 'sales','salary': 'low'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample,tree,default = 1):\n",
    "    for key in list(sample.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                prediction = tree[key][sample[key]] \n",
    "            except:\n",
    "                return default\n",
    "            prediction = tree[key][sample[key]]\n",
    "            if isinstance(prediction,dict):\n",
    "                return predict(sample,prediction)\n",
    "            else:\n",
    "                return prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to handle divide by zero error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function to calculate the statistics on the data.\n",
    "it prints the classification error, accuracy, precision, recall and F1 score\n",
    "on the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(data,tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
    "    TP,TN,FP,FN = 0,0,0,0\n",
    "    for i in range(len(data)):\n",
    "        if predicted[\"predicted\"].iloc[i] == 0.0:\n",
    "            if data['left'].iloc[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if data['left'].iloc[i] == 0:\n",
    "                FP += 1\n",
    "            else:\n",
    "                TP += 1\n",
    "    classification_error = safe_div((FP+FN),(TP+FP+TN+FN)) \n",
    "    accuracy = safe_div((TP+TN),(TP+FP+TN+FN)) \n",
    "    recall = safe_div(TP,(TP+FN))\n",
    "    precision = safe_div(TP,(TP+FP))\n",
    "    f1_score = safe_div(2,(safe_div(1,precision))+safe_div(1,recall))\n",
    "    #print(TP,TN,FP,FN)\n",
    "    print(\"Classification error:\",classification_error)\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"Precision:\",precision)\n",
    "    print(\"F1 Score:\",f1_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6742 2248 2248\n",
      "Performance on training data\n",
      "Classification error: 0.23524176802135865\n",
      "Accuracy: 0.7647582319786413\n",
      "Recall: 0.000630119722747322\n",
      "Precision: 1.0\n",
      "F1 Score: 0.0012594458438287153\n",
      "\n",
      "Performance on validation data\n",
      "Classification error: 0.24154804270462635\n",
      "Accuracy: 0.7584519572953736\n",
      "Recall: 0.001841620626151013\n",
      "Precision: 0.5\n",
      "F1 Score: 0.003669724770642202\n",
      "\n",
      "Performance on testing data\n",
      "Classification error: 0.24243772241992884\n",
      "Accuracy: 0.7575622775800712\n",
      "Recall: 0.003669724770642202\n",
      "Precision: 0.5\n",
      "F1 Score: 0.007285974499089253\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    training_data,validation_data,testing_data = train_validate_test_split(dataset)\n",
    "    print(len(training_data),len(validation_data),len(testing_data))\n",
    "    tree = createtree(training_data,training_data,training_data.columns[:-1])\n",
    "    print(\"Performance on training data\")\n",
    "    stats(training_data,tree)\n",
    "    print(\"\\nPerformance on validation data\")\n",
    "    stats(validation_data,tree)\n",
    "    print(\"\\nPerformance on testing data\")\n",
    "    stats(testing_data,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
