{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input_data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('xAttack',axis=1)\n",
    "y = df['xAttack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "X = (X-np.mean(X,axis=0))/(np.std(X,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintestvalidatesplit(data):\n",
    "    x,y = data[0], data[1]\n",
    "    n = x.shape[0]\n",
    "    k = int(n*0.8)\n",
    "    x_train = x[:k]\n",
    "    y_train = y[:k]\n",
    "    x_val = x[k:]\n",
    "    y_val = y[k:]\n",
    "    return [x_train,y_train],[x_val,y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (2/(1+np.exp(-2*x))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x):\n",
    "    return 1 - (x * x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,layerSizes,activation,learningrate = 0.01):\n",
    "        self.shape = layerSizes\n",
    "        self.activation = activation\n",
    "        self.learningrate = learningrate\n",
    "        self.activations = {'sigmoid':[sigmoid,sigmoid_derivative],\n",
    "                           'relu':[relu,relu_derivative],\n",
    "                           'tanh':[tanh,tanh_derivative],\n",
    "                           'linear':[linear,linear_derivative]}\n",
    "        n = len(layerSizes)\n",
    "        self.layers = []\n",
    "        self.layers.append(np.ones(self.shape[0]))\n",
    "        for i in range(1,n):\n",
    "            self.layers.append(np.ones(self.shape[i]))\n",
    "        self.weights = []\n",
    "        for i in range(n-1):\n",
    "            temp = np.zeros((self.layers[i].size,self.layers[i+1].size), dtype = 'd')\n",
    "            self.weights.append(np.random.randn(*temp.shape))\n",
    "        self.derivative = [0,]*len(self.weights)\n",
    "\n",
    "    def forwardpass(self,data):\n",
    "        self.layers[0] = data\n",
    "        for i in range(1,len(self.shape)):\n",
    "            self.layers[i][...] = self.activations[self.activation][0](np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "        return self.layers[-1]\n",
    "\n",
    "\n",
    "    def backpropogation(self, target, momentum=0.1):\n",
    "        error = target - self.layers[-1]\n",
    "        weight_deltas = []\n",
    "        weight_delta = error*self.activations[self.activation][1](self.layers[-1])\n",
    "        weight_deltas.append(weight_delta)\n",
    "\n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "            weight_delta = np.dot(weight_deltas[0],self.weights[i].T)*self.activations[self.activation][1](self.layers[i])\n",
    "            weight_deltas.insert(0,weight_delta)\n",
    "            \n",
    "        for i in range(len(self.weights)):\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            weight_delta = np.atleast_2d(weight_deltas[i])\n",
    "            der = np.dot(layer.T,weight_delta)\n",
    "            self.weights[i] += self.learningrate*der + momentum*self.derivative[i]\n",
    "            self.derivative[i] = der\n",
    "\n",
    "        return (error**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP(network,samples, epochs=10, momentum=0.1):\n",
    "    error_set = []\n",
    "    for i in range(epochs):\n",
    "        print('Epoch: ', i+1)\n",
    "        n = samples[0].shape[0]\n",
    "        error = 0\n",
    "        for j in range(n):\n",
    "            out = network.forwardpass(samples[0][j] )\n",
    "            error += network.backpropogation( samples[1][j], momentum )\n",
    "        error_set.append(error/n)\n",
    "        print('Training error',error/n)\n",
    "    return error_set, error/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training error 0.8737579085698337\n",
      "Epoch:  2\n",
      "Training error 0.873874092819078\n",
      "Epoch:  3\n",
      "Training error 0.874109560077602\n",
      "Epoch:  4\n",
      "Training error 0.8704756339086077\n",
      "Epoch:  5\n",
      "Training error 0.8756422834233158\n",
      "Epoch:  6\n",
      "Training error 0.8754237190389235\n",
      "Epoch:  7\n",
      "Training error 0.8811132232154536\n",
      "Epoch:  8\n",
      "Training error 0.883036349734231\n",
      "Epoch:  9\n",
      "Training error 0.8835228678992805\n",
      "Epoch:  10\n",
      "Training error 0.8837930587504453\n",
      "Epoch:  11\n",
      "Training error 0.8839652947882175\n",
      "Epoch:  12\n",
      "Training error 0.8840054670418389\n",
      "Epoch:  13\n",
      "Training error 0.8842223989408962\n",
      "Epoch:  14\n",
      "Training error 0.8843915760829952\n",
      "Epoch:  15\n",
      "Training error 0.8844020505489533\n",
      "Epoch:  16\n",
      "Training error 0.8844173543318687\n",
      "Epoch:  17\n",
      "Training error 0.88415935942823\n",
      "Epoch:  18\n",
      "Training error 0.8837051567203176\n",
      "Epoch:  19\n",
      "Training error 0.8837055865138633\n",
      "Epoch:  20\n",
      "Training error 0.8837183353204636\n",
      "Epoch:  21\n",
      "Training error 0.883713842032243\n",
      "Epoch:  22\n",
      "Training error 0.8837060238717211\n",
      "Epoch:  23\n",
      "Training error 0.8837130371428963\n",
      "Epoch:  24\n",
      "Training error 0.883711689957291\n",
      "Epoch:  25\n",
      "Training error 0.8837202961692213\n",
      "Epoch:  26\n",
      "Training error 0.8837750788075613\n",
      "Epoch:  27\n",
      "Training error 0.8837527083300353\n",
      "Epoch:  28\n",
      "Training error 0.8837440886923389\n",
      "Epoch:  29\n",
      "Training error 0.8837699577535731\n",
      "Epoch:  30\n",
      "Training error 0.8837688154384291\n",
      "Epoch:  31\n",
      "Training error 0.8837685834481872\n",
      "Epoch:  32\n",
      "Training error 0.8837905440667011\n",
      "Epoch:  33\n",
      "Training error 0.8837865805193713\n",
      "Epoch:  34\n",
      "Training error 0.8838062206916664\n",
      "Epoch:  35\n",
      "Training error 0.8837877654162565\n",
      "Epoch:  36\n",
      "Training error 0.8837884250241727\n",
      "Epoch:  37\n",
      "Training error 0.8837963589573049\n",
      "Epoch:  38\n",
      "Training error 0.8838080446095439\n",
      "Epoch:  39\n",
      "Training error 0.8838205276642533\n",
      "Epoch:  40\n",
      "Training error 0.8838229317636747\n",
      "Epoch:  41\n",
      "Training error 0.8838263788349972\n",
      "Epoch:  42\n",
      "Training error 0.8838297506351236\n",
      "Epoch:  43\n",
      "Training error 0.8838489635604124\n",
      "Epoch:  44\n",
      "Training error 0.8838657930842835\n",
      "Epoch:  45\n",
      "Training error 0.8838467816032018\n",
      "Epoch:  46\n",
      "Training error 0.8838700313320207\n",
      "Epoch:  47\n",
      "Training error 0.883889408799533\n",
      "Epoch:  48\n",
      "Training error 0.8839002044048199\n",
      "Epoch:  49\n",
      "Training error 0.8839102951112892\n",
      "Epoch:  50\n",
      "Training error 0.8838914823076071\n",
      "Epoch:  51\n",
      "Training error 0.8838822323823282\n",
      "Epoch:  52\n",
      "Training error 0.8839084600436484\n",
      "Epoch:  53\n",
      "Training error 0.8839023014478189\n",
      "Epoch:  54\n",
      "Training error 0.8839278896853927\n",
      "Epoch:  55\n",
      "Training error 0.8839082608299872\n",
      "Epoch:  56\n",
      "Training error 0.883922063028125\n",
      "Epoch:  57\n",
      "Training error 0.883919421366446\n",
      "Epoch:  58\n",
      "Training error 0.8839148069950137\n",
      "Epoch:  59\n",
      "Training error 0.8839196731843495\n",
      "Epoch:  60\n",
      "Training error 0.8839184212325509\n",
      "Epoch:  61\n",
      "Training error 0.8839301257201014\n",
      "Epoch:  62\n",
      "Training error 0.8839397549494507\n",
      "Epoch:  63\n",
      "Training error 0.8839143554664711\n",
      "Epoch:  64\n",
      "Training error 0.8839317862815692\n",
      "Epoch:  65\n",
      "Training error 0.8839481313722445\n",
      "Epoch:  66\n",
      "Training error 0.8839504840613334\n",
      "Epoch:  67\n",
      "Training error 0.8839548696601411\n",
      "Epoch:  68\n",
      "Training error 0.8839578242701781\n",
      "Epoch:  69\n",
      "Training error 0.883935428665023\n",
      "Epoch:  70\n",
      "Training error 0.8839398759563724\n",
      "Epoch:  71\n",
      "Training error 0.8839662422423649\n",
      "Epoch:  72\n",
      "Training error 0.8839521846600726\n",
      "Epoch:  73\n",
      "Training error 0.8839633570050435\n",
      "Epoch:  74\n",
      "Training error 0.8839518760507856\n",
      "Epoch:  75\n",
      "Training error 0.8839533783593175\n",
      "Epoch:  76\n",
      "Training error 0.8839520304939265\n",
      "Epoch:  77\n",
      "Training error 0.8839494867985754\n",
      "Epoch:  78\n",
      "Training error 0.8839620805433638\n",
      "Epoch:  79\n",
      "Training error 0.8839699301603734\n",
      "Epoch:  80\n",
      "Training error 0.8839744579103671\n",
      "Epoch:  81\n",
      "Training error 0.8839558853823264\n",
      "Epoch:  82\n",
      "Training error 0.883970044343609\n",
      "Epoch:  83\n",
      "Training error 0.8839630465175857\n",
      "Epoch:  84\n",
      "Training error 0.8839732920502895\n",
      "Epoch:  85\n",
      "Training error 0.8839789982512155\n",
      "Epoch:  86\n",
      "Training error 0.8839815619032121\n",
      "Epoch:  87\n",
      "Training error 0.8839870877635408\n",
      "Epoch:  88\n",
      "Training error 0.8839682595044334\n",
      "Epoch:  89\n",
      "Training error 0.8839942617653012\n",
      "Epoch:  90\n",
      "Training error 0.8839916746012229\n",
      "Epoch:  91\n",
      "Training error 0.8839979026996665\n",
      "Epoch:  92\n",
      "Training error 0.8839640578180498\n",
      "Epoch:  93\n",
      "Training error 0.8839867700967746\n",
      "Epoch:  94\n",
      "Training error 0.8839890060738128\n",
      "Epoch:  95\n",
      "Training error 0.8839825099116809\n",
      "Epoch:  96\n",
      "Training error 0.8839990209447965\n",
      "Epoch:  97\n",
      "Training error 0.8840015793127443\n",
      "Epoch:  98\n",
      "Training error 0.8839979570961373\n",
      "Epoch:  99\n",
      "Training error 0.88400199176647\n",
      "Epoch:  100\n",
      "Training error 0.8840028440272167\n"
     ]
    }
   ],
   "source": [
    "nn = NN([X.shape[1],14,X.shape[1]],'relu',0.01)\n",
    "epochs = 100\n",
    "error_set, finalerror = trainMLP(nn,[X,X],epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = pd.DataFrame(np.dot(X,nn.weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data.insert(loc=13, column='class', value=y)\n",
    "reduced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reduced_data).to_csv(\"../input_data/reducedData_a.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
